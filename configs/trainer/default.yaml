default_root_dir: ${oc.env:AMLT_OUTPUT_DIR,outputs}

gpus: 1
num_nodes: 1
accelerator: gpu
strategy: ddp_find_unused_parameters_false

min_epochs: 1
max_epochs: 10
enable_progress_bar: true

sync_batchnorm: True
enable_checkpointing: True
resume_from_checkpoint: null

# debugging
fast_dev_run: false

logger:
  class_path: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
  init_args:
    save_dir: ${default_root_dir}/logs
    name: null
    version: null
    log_graph: False
    default_hp_metric: True
    prefix: ""

callbacks:
  - class_path: pytorch_lightning.callbacks.ModelCheckpoint
    init_args:
      dirpath: "${default_root_dir}/checkpoints/"
      monitor: "val/acc" # name of the logged metric which determines when model is improving
      mode: "max" # "max" means higher metric value is better, can be also "min"
      save_top_k: 1 # save k best models (determined by above metric)
      save_last: True # additionaly always save model from last epoch
      verbose: False
      filename: "epoch_{epoch:03d}"
      auto_insert_metric_name: False

  - class_path: pytorch_lightning.callbacks.EarlyStopping
    init_args:
      monitor: "val/acc" # name of the logged metric which determines when model is improving
      mode: "max" # "max" means higher metric value is better, can be also "min"
      patience: 100 # how many validation epochs of not improving until training stops
      min_delta: 0 # minimum change in the monitored metric needed to qualify as an improvement

  - class_path: pytorch_lightning.callbacks.RichModelSummary
    init_args:
      max_depth: -1

  - class_path: pytorch_lightning.callbacks.RichProgressBar
